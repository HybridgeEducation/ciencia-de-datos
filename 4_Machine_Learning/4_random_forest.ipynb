{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest\n",
    "\n",
    "## El problema\n",
    "\n",
    "Los árboles de decisión nos dejan con una decisión difícil:\n",
    "- Un árbol profundo con muchas hojas sobreajustará porque cada predicción proviene de datos históricos de solo las pocas casas en su hoja.\n",
    "- Un árbol poco profundo con pocas hojas tendrá un rendimiento deficiente porque no captura tantas distinciones en los datos en bruto.\n",
    "\n",
    "Incluso las técnicas de modelado más sofisticadas de hoy en día enfrentan esta tensión entre el subajuste y el sobreajuste. Sin embargo, muchos modelos tienen ideas ingeniosas que pueden conducir a un mejor rendimiento. Veremos el Random Forest (bosque aleatorio) como un ejemplo.\n",
    "\n",
    "<img src=\"./img/random_vs_single.png\" alt=\"random_vs_single\" width=\"600\">\n",
    "\n",
    "El Random Forest utiliza muchos árboles y hace una predicción promediando las predicciones de cada árbol. Generalmente, tiene una precisión predictiva mucho mejor que un solo árbol de decisión y funciona bien con parámetros predeterminados.\n",
    "\n",
    "---\n",
    "## Implementación\n",
    "\n",
    "Veamos... vovleremos a preparar nuestros datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "# Cargar datos\n",
    "ruta = './data/melb_data.csv'\n",
    "melbourne_data = pd.read_csv(ruta) \n",
    "\n",
    "# Quitar nulos\n",
    "melbourne_data_filtrada = melbourne_data.dropna(axis=0)\n",
    "\n",
    "# Elegir target (crear y)\n",
    "y = melbourne_data_filtrada.Price\n",
    "\n",
    "# Elegir features\n",
    "melbourne_features = ['Rooms', 'Bathroom', 'Landsize', 'BuildingArea', 'YearBuilt', 'Lattitude', 'Longtitude']\n",
    "\n",
    "# Crear X\n",
    "X = melbourne_data_filtrada[melbourne_features]\n",
    "\n",
    "train_X, val_X, train_y, val_y = train_test_split(X, y, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y ahora vamos a constuir el modelo Random Forest. Lo primero que notaremos es que importaremos una clase distinta de Scikit Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bosque = RandomForestRegressor(random_state=1)\n",
    "bosque.fit(train_X, train_y)\n",
    "preds = bosque.predict(val_X)\n",
    "print(mean_absolute_error(val_y, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusión\n",
    "\n",
    "Probablemente haya margen para mejorar aún más, pero esto es una gran mejora en comparación con el mejor error del árbol de decisión de 250,000. Hay parámetros que nos permiten cambiar el rendimiento del Bosque Aleatorio de manera similar a como cambiamos la profundidad máxima del árbol de decisión individual. Pero una de las mejores características de los modelos de Bosque Aleatorio es que generalmente funcionan razonablemente bien incluso sin este ajuste."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
